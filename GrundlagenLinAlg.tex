\chapter{Grundlagen aus der linearen Algebra}
\section{Norm und Kondition}
Zwei Matrixnormen werden im Laufe der Arbeit von entscheidener Bedeutung sein,
weshalb wir uns zunächst an deren Definitionen erinnern.

\begin{mydef}[Frobenius- und Zeilensummennorm]
    Sei $A = (a_{kj})_{k,j=0}^{n-1} \in \C^{n\times n}$ eine Matrix.
    Die \emph{Frobeniusnorm} von $A$ ist definiert durch
    \begin{equation}
        \label{eq:frobenius_norm}
        \norm{A}_F
        \defeq \sqrt{ \sum_{k=0}^{n-1} \sum_{j=0}^{n-1} \abs{a_{kj}}^2}.
    \end{equation}

    \noindent Die \emph{Zeilensummennorm} von $A$ ist definiert durch
    \begin{equation}
        \label{eq:infinity_norm}
        \norm{A}_\infty
        \defeq \max_{k=0, \dots, n-1} \sum_{j=0}^{n-1} \abs{a_{kj}}.
    \end{equation}
\end{mydef}

% TODO: Proof unitary invariance of frobenius norm?!
\begin{remark}
    Später werden wir die \emph{unitäre Invarianz} der
    Frobeniusnorm benötigen, d.h. die Eigenschaft, dass für eine unitäre Matrix
    $U \in \C^{n\times n}$ und eine beliebige Matrix ${A \in \C^{n\times n}}$
    gilt:
    \[
        \norm{A U}_F = \norm{U A}_F = \norm{A}_F.
    \]
\end{remark}

Im Folgenden wird ähnlich wie in \cite[S. 205ff]{stoer1} der Begriff der Kondition
einer Matrix motiviert.  Dazu seien eine Vektorraumnorm auf $\Cn$ und eine
submultiplikative Matrixnorm auf $\C^{n\times n}$ gegeben, die wir jeweils mit
$\emptynorm$ bezeichnen. Die Matrixnorm sei dabei mit der Vektorraumnorm
verträglich, d.h. es gelte $ \norm{Ax} \leq \norm{A}\norm{x}$ für alle $A \in
\C^{n\times n}$ und $x \in \Cn$.
Wir betrachten ein lineares Gleichungssystem der Form
$Ax = b$ mit invertierbarer Matrix
$A \in \C^{n \times n}$, $n \in \N$, $b \in \Cn$
und gesuchtem Vektor $x \in \Cn$.
Dabei nehmen wir $x \neq 0$ und $b \neq 0$ an.
% TODO : Eingangsvektor?
Weiter sei ein verfälschter Eingangsvektor $\tilde{b} = b + \Delta b$,
$\Delta b \in \Cn$ mit relativem Fehler
\[
    \frac{\norm{\tilde{b} - b}}{\norm{b}} = \frac{\norm{\Delta b}}{\norm{b}} \leq \delta
\]
gegeben.
Wir bezeichnen mit $\tilde{x} \in \Cn$ die Lösung des verfälschten Gleichungssystems
$A \tilde{x} = \tilde{b}$.
Mit $\Delta x \defeq A^{-1} \Delta b$ erhalten wir wegen der Linearität von $A^{-1}$
\[
    \tilde{x} = A^{-1} \tilde{b} = A^{-1} b + A^{-1} \Delta b = x + \Delta x.
\]

\noindent Gesucht ist nun ein Maß des relativen Fehlers der verfälschten Lösung
$\tilde{x}$ in Abhängigkeit vom Eingangsfehler $\delta$.
Wegen $ \norm{\Delta x} = \norm{A^{-1} \Delta b} \leq \norm{A^{-1}} \norm{\Delta b} $
und $ \norm{b} = \norm{A x} \leq \norm{A} \norm{x} $,
folgt für den relativen Fehler
%FIXME: equal sized norm-bars
\[
    \frac{\norm{\Delta x}}{\norm{x}}
    \leq \norm{A} \norm{A^{-1}} \frac{\norm{\Delta b}}{\norm{b}}
    \leq \norm{A} \norm{A^{-1}} \delta.
\]

\noindent Dies motiviert die folgende
\begin{mydef}[Kondition einer Matrix]
    Sei $A \in \C^{n \times n}$.
    Dann ist die \emph{Kondition von $A$ bezüglich der Norm $\emptynorm$} durch
    $\cond{A} \defeq \norm{A} \norm{A^{-1}}$
    definiert.
\end{mydef}

\begin{remark}
    Mit den Bezeichnungen wie oben gilt dann ${\frac{\norm{\Delta x}}{\norm{x}} \leq \cond{A} \cdot \delta }$.
\end{remark}

\begin{remark}
    Auch für den Fall, dass die Matrix $A$ in verfälschter Form $A + \Delta A$
    vorliegt und somit das Gleichungssystem $(A + \Delta A)(x + \Delta x) = (b
    + \Delta b)$ gelöst wird, lässt sich eine Ungleichung mit Hilfe der
    Konditionszahl herleiten. Dazu sei auf \cite[S. 203ff]{stoer1} und \cite[S.
    54ff]{schaback} verwiesen.
\end{remark}

\begin{notation}
    Für die Konditionen bezüglich der Frobenius- und der Zeilensummennorm
    verwenden wir die Schreibweisen $ \cond[F]{A} $ bzw. $\cond[\infty]{A}$ für
    Matrizen $A \in \C^{n\times n}$.
\end{notation}

\section{Vandermonde-Matrizen}

\begin{mydef}[Vandermonde-Matrix]
    Für $n \in \N$ und einen Vektor ${z = (z_0, \dots, z_{n-1}) \in \Cn}$
    sei die
    \emph{Vandermonde-Matrix zu den Stützstellen (oder Knoten) $z_0, \dots, z_{n-1}$}
    durch
    \[
        \Vand{z} \defeq \begin{pmatrix}
            1         & 1         & \dots & 1 \\
            z_0       & z_1       & \dots & z_{n-1} \\
            z_0^2     & z_1^2     & \dots & z_{n-1}^2 \\
            \vdots    & \vdots    & \ddots & \vdots \\
            z_0^{n-1} & z_1^{n-1} & \dots & z_{n-1}^{n-1}
        \end{pmatrix} \in \C^{n \times n}
    \]
    definiert.
    Bezeichnet man die Komponenten der Vandermonde-Matrix mit
    $v_{kj} \in \C$, so ergibt sich
    $ v_{kj} = z_j^k $ für $k, j = 0, \dots, n-1$.

\end{mydef}

\begin{lemma}
    Sei $z = (z_0, \dots, z_{n-1}) \in \Cn$.
    Es gilt
    \[
        \det \Vand{z}
        = \prod_{0 \leq k < j \leq n-1} \left( z_j - z_k \right).
    \]
\end{lemma}

\begin{proof}
    Bezeichne mit $v_{kj} = z_j^k$ für $k,j = 0, \dots, n-1$ die Elemente von $\Vand{z}$.
    Der Beweis erfolgt durch vollständige Induktion nach $n \in \N$.\\
    \textbf{Induktionsanfang (\boldmath $n=1$):}\\
    Da das leere Produkt per Definition $1$ ergibt, gilt
    $\det{\Vand{z}}~=~1~=~\prod_{0 \leq k < j \leq 0} (z_j~-~z_k)$. \\[0.5em]
    \textbf{Induktionvoraussetzung:}
    Sei die Behauptung für $n-1 \in \N$ erfüllt.\\[0.5em]
    \textbf{Induktionsschritt (\boldmath $n\!-\!1 \rightarrow n$):}
    Durch Zeilenoperationen ändert sich der Wert der Determinante nicht.
    Wir ziehen daher in jeder Zeile das $z_0$-fache der vorherigen Zeile ab und
    erhalten
    \[
        \det{\Vand{z}}
        = \begin{vmatrix}
            1         & 1         & \dots & 1 \\
            z_0       & z_1       & \dots & z_{n-1} \\
            z_0^2     & z_1^2     & \dots & z_{n-1}^2 \\
            \vdots    & \vdots    & \ddots & \vdots \\
            z_0^{n-1} & z_1^{n-1} & \dots & z_{n-1}^{n-1}
        \end{vmatrix}
        = \begin{vmatrix}
            1      & 1                  & \dots & 1 \\
            0      & (z_1-z_0)          & \dots & (z_{n-1}-z_0) \\
            0      & (z_1-z_0)z_1^1     & \dots & (z_{n-1}-z_0) z_{n-1}^1 \\
            \vdots & \vdots             & \ddots & \vdots \\
            0      & (z_1-z_0)z_1^{n-2} & \dots & (z_{n-1}-z_0) z_{n-1}^{n-2}
        \end{vmatrix}.
    \]
    Die Entwicklung der Determinante nach der ersten Spalte mit Hilfe des
    Laplace'schen Entwicklungssatzes und Ausnutzung der Multilinearität der
    Determinante liefern nun die Behauptung:
    \[
        \begin{split}
            \det{\Vand{z}}
            &= \prod_{r=0}^{n-1} (z_r - z_0) \begin{vmatrix}
                1         & \dots & 1 \\
                z_1       & \dots & z_{n-1} \\
                z_1^2     & \dots & z_{n-1}^2 \\
                \vdots    & \ddots & \vdots \\
                z_1^{n-2} & \dots & z_{n-1}^{n-2}
            \end{vmatrix}
            = \prod_{r=0}^{n-1} (z_r - z_0) \det{\Vand{z_1, \dots, z_{n-1}}}\\
            &\overset{IV}{=}
                \prod_{r=0}^{n-1} (z_r - z_0) \prod_{1 \leq k < j \leq n-1} (z_j - z_k)
            = \prod_{0 \leq k < j \leq n-1} (z_j - z_k).
        \end{split}
    \]
\end{proof}

\begin{corollary}
    Die Vandermonde-Matrix $\Vand{z}$ mit
    $z = (z_0, \dots, z_{n-1}) \in \Cn$ ist genau dann invertierbar, wenn
    $z_k \neq z_j$ für alle $k \neq j$ gilt, d.h. wenn die $z_k$ paarweise
    verschieden sind.
\end{corollary}

%TODO: Überleitung?
\begin{lemma}
    \label{lemma:vandermonde_const_multiplication}
    Sei $z = (z_0, \dots, z_{n-1}) \in \Cn$ mit $z_k$ paarweise verschieden und
    $\alpha \in \C \! \setminus \! \{0\}$.
    Dann gilt
    \begin{subequations}
        \begin{equation}
            \label{eq:vandermonde_const_multiplication}
            \Vand{\alpha z}
            = \diag{\alpha^0, \dots, \alpha^{n-1}} \cdot \Vand{z}
        \end{equation}
        und entsprechend
        \begin{equation}
            \label{eq:inverse_vandermonde_const_multiplication}
            \Vand{\alpha z}^{-1}
            = \Vand{z}^{-1} \cdot \diag{\alpha^0, \alpha^{-1}, \dots, \alpha^{-n+1}}.
        \end{equation}
    \end{subequations}
\end{lemma}
\begin{proof}
    Wir setzen
    \[
        V = (v_{kj})_{k,j=0}^{n-1} \defeq \Vand{z}, \;
        \tilde{V} = (\tilde{v}_{kj})_{k,j=0}^{n-1} \defeq \Vand{\alpha z}.
    \]

    \noindent Dann gilt für $j,k = 0,\dots,n-1$
    \[
        \tilde{v}_{kj} = (\alpha z_j)^k = \alpha^k z_j^k = \alpha^k v_{kj},
    \]
    d.h. wir können
    \[
        \tilde{V} = \diag{\alpha^0, \dots, \alpha^{n-1}} \cdot V
    \]
    schreiben.
    Mit $z_0, \dots, z_{n-1} \in \C$ sind auch
    $\alpha z_0, \dots, \alpha z_{n-1} \in \C$
    paarweise verschieden, so dass $\tilde{V}$ invertierbar ist und es folgt
    \[
        \tilde{V}^{-1}
        = \left( \diag{\alpha^0, \dots, \alpha^{n-1}} \cdot V \right)^{-1}
        = V^{-1} \cdot \diag{\alpha^0, \alpha^{-1}, \dots, \alpha^{-n+1}}.
    \]
\end{proof}

