\chapter{Elementarsymmetrische Polynome}

In diesem Abschnitt führen wir die \emph{elementarsymmetrischen Polynome} ein.
Diese liefern uns später eine explizite Darstellung der inversen
Vandermonde-Matrix und sind Grundlage für den Beweis einer oberen Schranke von
$\norm{V^{-1}}_\infty$.

\section{Definition und Eigenschaften}

\begin{mydef}[Elementarsymmetrische Polynome]
    Für $x = (x_1, \dots, x_{n}) \in \Cn$ definieren wir die
    \emph{r-ten elementarsymmetrischen Polynome
    $\sigma_{r}(x) = \sigma_{r}(x_1, \dots, x_{n})$ mit $r = 0, \dots, n$
    in den $n$ Variablen $x_j$ mit $j = 1, \dots, n$} durch die Koeffizienten
    des Polynoms
    \[
        p(z)
        = \prod_{k=1}^{n} (z + x_k)
        \eqdef \sum_{r=0}^{n} \sigma_{r}(x_1, \dots, x_{n}) z^{n-r}.
    \]
\end{mydef}

\begin{notation}
    Als abkürzende Schreibweise setzen wir für gegebenes
    $x = (x_1, \dots, x_{n}) \in \Cn$
    \[
        \sigma_{r}^{j}(x) \defeq \sigma_{r}(x_1, \dots, x_{j-1}, x_{j+1}, \dots, x_{n}).
    \]
    Damit ist $\sigma_{r}^{j}(x)$ das r-te elementarsymmetrische Polynom in den
    $n-1$ Variablen $x_k$ mit $k \in \{ 1, \dots, n \} \setminus \{ j \}$.
\end{notation}

\begin{remark}
    Ausmultiplizieren des Polynoms und Koeffizientenvergleich liefern:
    \begin{equation}
        \label{eq:explicit_elementary_symmetric_polynomials}
        \sigma_{r}(x_1, \dots, x_{n})
        = \sum_{1 \leq j_1 < \dots < j_r \leq n} x_{j_1} \cdots x_{j_r}
        = \sum_{1 \leq j_1 < \dots < j_r \leq n} \left( \prod_{k=1}^r x_{j_k} \right)
    \end{equation}
    für $r \in \N$.
    Dabei ist der Fall $r = 0$ nicht als leere Summe, sondern als Summe mit
    leerem Produkt als einzigem Summanden zu verstehen, so dass
    $\sigma_0(x) = 1$ gilt. Dies entspricht gerade dem Höchstkoeffizienten des
    Polynoms aus der Definition.
\end{remark}

\begin{lemma}
    \label{lemma:recursion_elementary_symmetric_polynomials}
    Die elementarsymmetrischen Polynome erfüllen die Rekursionsformel:
    \begin{equation}
        \label{eq:recursion_elementary_symmetric_polynomials}
        \sigma_r (x_1, \dots, x_{n}) = \sigma_r (x_1, \dots, x_{n-1}) + x_{n} \sigma_{r-1} (x_1, \dots, x_{n-1}).
    \end{equation}
\end{lemma}
\begin{proof}
    Wir verwenden Gleichung (\ref{eq:explicit_elementary_symmetric_polynomials})
    und teilen die rechte Seite in zwei Gruppen von Summanden auf, je nachdem,
    ob $x_n$ ein Faktor im Summand ist:
    \begin{equation*}
        \begin{split}
            \sigma_r (x_1, \dots, x_{n})
            &= \sum_{1 \leq j_1 < \dots < j_r \leq n} x_{j_1} \cdots x_{j_r}\\
            &= \sum_{1 \leq j_1 < \dots < j_r \leq n-1} x_{j_1} \cdots x_{j_r}
            + \sum_{1 \leq j_1 < \dots < j_{r-1} \leq n-1} x_{j_1} \cdots x_{j_r} \cdot x_{n}\\
            &= \sigma_r (x_1, \dots, x_{n-1}) + x_{n} \sigma_{r-1} (x_1, \dots, x_{n-1}).
        \end{split}
    \end{equation*}
\end{proof}

\noindent Im Folgenden beweisen wir noch zwei technische Lemmata, die später
nützlich sind.

\begin{lemma}
    \label{lemma:elementary_symmetric_polynomials_const_multiplication}
    Für $x = (x_1, \dots, x_n) \in \Cn$ und $\alpha \in \C$ gilt:
    \begin{equation}
        \label{eq:elementary_symmetric_polynomials_const_multiplication}
        \sigma_{r}(\alpha x)
        = \alpha^{r} \sigma_{r}(x).
    \end{equation}
\end{lemma}
\begin{proof}
    Unter Verwendung von Gleichung
    (\ref{eq:explicit_elementary_symmetric_polynomials})
    erhalten wir
    \[
        \sigma_{r}(\alpha x)
        = \sum_{1 \leq j_1 < \dots < j_r \leq n} \alpha x_{j_1} \cdots \alpha x_{j_r}
        = \alpha^r \sum_{1 \leq j_1 < \dots < j_r \leq n} x_{j_1} \cdots x_{j_r}
        = \alpha^r \sigma_{r}(x).
    \]
\end{proof}

\begin{lemma}
    \label{lemma:outlier_sigma_zero}
    Bezeichnen wir für $n \in \N$ die erste $n$-te Einheitswurzel mit
    $\wn \defeq e^{\frac{2 \pi i}{n}}$, so gilt
    \begin{equation}
        \label{eq:outlier_sigma_zero}
        \abs{\sigma_r(\wn^1, \dots, \wn^{n-1})} = 1
    \end{equation}
    für alle $r = 0, \dots, n-1$.
\end{lemma}
\begin{proof}
    Nach Lemma \ref{lemma:elementary_symmetric_polynomials_const_multiplication}
    gilt
    \[
        \abs{\sigma_r(-\wn^1, \dots, -\wn^{n-1})}
        = \abs{(-1)^r \sigma_r(\wn^1, \dots, \wn^{n-1})}
        = \abs{\sigma_r(\wn^1, \dots, \wn^{n-1})}.
    \]
    Nach Definition der elementarsymmetrischen Polynome ist
    $\sigma_r({-\wn^1, \dots, -\wn^{n-1}})$
    der $n\!-\!r\!-\!1\text{-te}$ Koeffizient des Polynoms vom
    Grad $n-1$, das eindeutig durch die $n-1$ Nullstellen
    $\wn^k$ mit $k=1, \dots, n-1$ bestimmt ist.
    Wir zeigen, dass dies das Polynom $p(z) = \sum_{r=0}^{n-1} z^r$ ist.

    \noindent Tatsächlich ist $\wn^k = e^{2 \pi i k / n}$ für $k = 1, \dots, n-1$
    eine Nullstelle von $p(z)$, denn wegen $\wn^k \neq 1$ folgt mit Hilfe der
    geometrischen Reihe
    \[
        \begin{split}
            p(\wn^k)
            &= \sum_{r=0}^{n-1} \left( \wn^k \right)^r
            = \frac{1 - \left( \wn^k \right)^n}{1 - \left( \wn^k \right)}
            = \frac{1 - \left( \wn^n \right)^k}{1 - \left( \wn^k \right)}
            = 0.
        \end{split}
    \]
    Damit ist gezeigt, dass für $r=0, \dots, n-1$
    \[
        \abs{\sigma_r(\wn^1, \dots, \wn^{n-1})}
        = \abs{\sigma_r(-\wn^1, \dots, -\wn^{n-1})}
        = 1
    \]
    gilt.
\end{proof}

\begin{corollary}
    \label{corollary:outlier_sigma_zero_row_sum}
    Mit $\omega_n \defeq e^{\frac{2 \pi i}{n}}$ gilt
    \begin{equation}
        \label{eq:outlier_sigma_zero_row_sum}
        \sum_{r=0}^{n-1} \abs{\sigma_r(\wn^1, \dots, \wn^{n-1})}
        \overset{\eqref{eq:outlier_sigma_zero}}= \sum_{r=0}^{n-1} 1
        = n.
    \end{equation}
\end{corollary}

\section{Eine Abschätzung der Betragssumme elementarsymmetrischer Polynome}

Die folgende Abschätzung liefert uns im nächsten Abschnitt eine obere Schranke
der Zeilensummennorm inverser Vandermonde-Matrizen.
\begin{lemma}[\cite{gautschi4}]
    \label{lemma:elementary_symmetric_polynomials_inequality}
    Sei $x = (x_1, \dots, x_n) \in \Cn$ wie zuvor.
    Dann gilt
    \begin{equation}
        \label{eq:elementary_symmetric_polynomials_inequality}
        \sum_{k=0}^{n} \abs{\sigma_{k}(x)} \leq \prod_{k=1}^{n} \left( 1 + \abs{x_k} \right).
    \end{equation}
    Gleichheit gilt, wenn $x_k = r_k \cdot e^{i\varphi}$ für ein
    festes $\varphi \in \R$ und beliebige $r_k \in \R_+$,
    d.h. wenn alle $x_k$ auf einer Halbgeraden durch den Nullpunkt
    in der komplexen Ebene liegen.
\end{lemma}

\begin{remark}
    Für die letzte Aussage des Lemmas lässt sich sogar Äquivalenz zeigen, d.h.
    Gleichheit gilt \emph{genau dann}, wenn $x_k = r_k \cdot e^{i\varphi}$ für
    ein festes $\varphi \in \R$ und beliebige $r_k \in \R_+$.
    Da diese Aussage jedoch für die vorliegende Arbeit nicht von Bedeutung ist,
    beweisen wir nur die schwächere Form des Lemmas.
\end{remark}

\begin{proof}[Beweis des Lemmas \ref{lemma:elementary_symmetric_polynomials_inequality}]
    Zum Beweis verwenden wir vollständige Induktion nach $n \in \N$ und
    Lemma \ref{lemma:recursion_elementary_symmetric_polynomials} an.\\[0.5em]
    \textbf{Induktionsanfang (\boldmath$n=1$):}
    Es ist $ (z+x_0) = \sigma_0(x_0) \cdot z^1 + \sigma_1(x_0) \cdot z^0$,
    also $\sigma_0(x_0) = 1$ und $\sigma_1(x_0) = x_0$.
    Damit ergibt sich: $\sum_{k=0}^{1} \abs{\sigma_k(x_0)} = \abs{1} + \abs{x_0} = 1 + \abs{x_0}$.\\[0.5em]
%
    \noindent \textbf{Induktionvoraussetzung:}
    Sei die Behauptung für $n-1 \in \N$ erfüllt.\\[0.5em]
%
    \noindent \textbf{Induktionsschritt (\boldmath $n\!-\!1 \rightarrow n$):}
    Unter Verwendung von $\sigma_{0}(x_1, \dots, x_n) = 1$
    und $\sigma_{n}(x_1, \dots, x_n) = x_1 \cdots x_n$ folgt:
    \begin{equation*}
      \begin{split}
        \prod_{k=1}^{n} \left( 1 + \abs{x_k} \right)
        &= ( 1 + \abs{x_n} ) \cdot \prod_{k=1}^{n-1} \left( 1 + \abs{x_k} \right)\\
        &\overset{IV}{\geq}  ( 1 + \abs{x_n} ) \cdot \sum_{k=0}^{n-1} \abs{\sigma_{k}(x_1, \dots, x_{n-1})}\\
        &= \sum_{k=0}^{n-1} \abs{\sigma_{k}(x_1, \dots, x_{n-1})}
        + \sum_{k=1}^{n} \abs{x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})}\\
        &\geq \sum_{k=1}^{n-1} \abs{\sigma_{k}(x_1, \dots, x_{n-1}) + x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})} + \abs{\sigma_{0}(x_1, \dots, x_{n-1})}\\
        &+ \abs{x_n \cdot \sigma_{n-1}(x_1, \dots, x_{n-1})}\\
        &\overset{(\ref{eq:recursion_elementary_symmetric_polynomials})}{=}
        \sum_{k=1}^{n-1} \abs{\sigma_{k}(x_1, \dots, x_{n})} + \abs{\sigma_{0}(x_1, \dots, x_{n})} + \abs{\sigma_{n}(x_1, \dots, x_n)}\\
        &= \sum_{k=0}^{n} \abs{\sigma_{k}(x_1, \dots, x_{n-1})}.
      \end{split}
    \end{equation*}

    Um den Spezialfall $x_k = r_k \cdot e^{i \varphi}$ der Behauptung zu beweisen,
    führen wir die Induktion analog durch, allerdings mit der neuen
    Induktionsbehauptung, dass Gleichheit gelte.  Es bleibt nur zu zeigen, dass
    die zweite obige Abschätzung zur Gleichheit wird, d.h. dass
    $\sigma_{k}(x_1, \dots, x_{n-1})$ und $x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})$
    linear abhängig sind und die Dreiecksungleichung mit Gleichheit
    \[
        \begin{split}
            \abs{\sigma_{k}(x_1, \dots, x_{n-1})} + \abs{x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})}\\
            = \abs{\sigma_{k}(x_1, \dots, x_{n-1}) + x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})}
        \end{split}
    \]
    gilt.

    \noindent Mit Gleichung
    (\ref{eq:explicit_elementary_symmetric_polynomials})
    sieht man sofort ein, dass
    $\sigma_k(r_1, \dots, r_{n-1}) \in \R_{+}$ für alle $k \in \N$ gilt.
    Weiter verwenden wir
    \lemmaref{elementary_symmetric_polynomials_const_multiplication}
    und stellen fest, dass
    \begin{equation*}
        \sigma_{k}(x_1, \dots, x_{n-1})
        = \sigma_k(e^{i \varphi} \cdot (r_1, \dots, r_{n-1}))
        \overset{(\ref{eq:explicit_elementary_symmetric_polynomials})}{=}
            e^{k i \varphi} \cdot \underbrace{\sigma_k(r_1, \dots, r_{n-1})}_{> 0}
    \end{equation*}
    und
    \begin{equation*}
        x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})
        = x_n \cdot \sigma_{k-1}(e^{i \varphi} \cdot (r_1, \dots, r_{n-1}))
        \overset{(\ref{eq:explicit_elementary_symmetric_polynomials})}{=}
            e^{k i \varphi} \cdot \underbrace{r_n \sigma_{k-1}(r_1, \dots, r_{n-1})}_{> 0}
    \end{equation*}
    \enlargethispage{2em}
    in die gleiche Richtung $e^{k i\varphi}$ zeigen, also linear abhängig
    sind.
\end{proof}
