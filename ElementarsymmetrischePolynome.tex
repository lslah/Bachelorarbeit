\chapter{Elementarsymmetrische Polynome}

In diesem Abschnitt führen wir die \emph{elementarsymmetrischen Polynome} ein.
Diese liefern uns später eine explizite Darstellung der inversen
Vandermonde-Matrix und sind Grundlage für den Beweis einer oberen Schranke von
$\norm{V^{-1}}_\infty$.

\section{Definition und Eigenschaften}

\begin{mydef}[Elementarsymmetrische Polynome]
    Für $x = (x_1, \dots, x_{n}) \in \Cn$ definieren wir die
    \emph{k-ten elementarsymmetrischen Polynome
    $\sigma_{k}(x) = \sigma_{k}(x_1, \dots, x_{n})$, $k = 0, \dots, n$
    in den $n$ Variablen $x_j$, $j = 1, \dots, n$} durch
    \[
        \prod_{k=1}^{n} (z + x_k)
        = \sum_{k=0}^{n} \sigma_{k}(x_1, \dots, x_{n}) z^{n-k}.
    \]
\end{mydef}

\begin{remark}
    Ausmultiplizieren des Polynoms und Koeffizientenvergleich liefern:
    \begin{equation}
        \label{eq:explicit_elementary_symmetric_polynomials}
        \sigma_{k}(x_1, \dots, x_{n})
        = \sum_{1 \leq j_1 < \dots < j_k \leq n} x_{j_1} \cdots x_{j_k}
        = \sum_{1 \leq j_1 < \dots < j_k \leq n} \left( \prod_{r=1}^k x_{j_r} \right).
    \end{equation}
\end{remark}

\begin{lemma}
    \label{lemma:recursion_elementary_symmetric_polynomials}
    Die elementarsymmetrischen Polynome erfüllen die Rekursionsformel:
    \begin{equation}
        \label{eq:recursion_elementary_symmetric_polynomials}
        \sigma_k (x_1, \dots, x_{n}) = \sigma_k (x_1, \dots, x_{n-1}) + x_{n} \sigma_{k-1} (x_1, \dots, x_{n-1}).
    \end{equation}
\end{lemma}

\begin{proof}
    Wir verwenden Gleichung (\ref{eq:explicit_elementary_symmetric_polynomials})
    und teilen die rechte Seite in zwei Gruppen von Summanden, je nachdem, ob
    $x_n$ ein Faktor im Summand ist:
    \begin{equation*}
        \begin{split}
            \sigma_k (x_1, \dots, x_{n})
            &= \sum_{1 \leq j_1 < \dots < j_k \leq n} x_{j_1} \cdots x_{j_k}\\
            &= \sum_{1 \leq j_1 < \dots < j_k \leq n-1} x_{j_1} \cdots x_{j_k}
            + \sum_{1 \leq j_1 < \dots < j_{k-1} \leq n-1} x_{j_1} \cdots x_{j_k} \cdot x_{n}\\
            &= \sigma_k (x_1, \dots, x_{n-1}) + x_{n} \sigma_{k-1} (x_1, \dots, x_{n-1}).
        \end{split}
    \end{equation*}
\end{proof}

\begin{notation}
    Als abkürzende Schreibweise setzen wir im Folgenden für gegebene
    $x_1, \dots, x_{n} \in \C$
    \[
        \sigma_{k}^{j} \defeq \sigma_{k}(x_1, \dots, x_{j-1}, x_{j+1}, \dots, x_{n}).
    \]
    Damit ist $\sigma_{k}^{j}$ das k-te elementarsymmetrische Polynom in den
    $n-1$ Variablen $x_r$, $r \in \{ 1, \dots, n \} \setminus \{ j \}$.
\end{notation}

\begin{lemma}
    \label{lemma:elementary_symmetric_polynomials_const_multiplication}
    Für $x = (x_1, \dots, x_n) \in \Cn$ und $\alpha \in \C$ gilt:
    \begin{equation}
        \label{eq:elementary_symmetric_polynomials_const_multiplication}
        \sigma_{k}(\alpha x)
        = \alpha^{k} \sigma_{k}(x)
    \end{equation}
\end{lemma}
\begin{proof}
    Unter Verwendung von Gleichung
    (\ref{eq:explicit_elementary_symmetric_polynomials})
    erhalten wir:
    \[
        \sigma_{k}(\alpha x)
        = \sum_{1 \leq j_1 < \dots < j_k \leq n} \alpha x_{j_1} \cdots \alpha x_{j_k}
        = \alpha^k \sum_{1 \leq j_1 < \dots < j_k \leq n} x_{j_1} \cdots x_{j_k}
        = \alpha^k \sigma_{k}(x).
    \]
\end{proof}

Wir beweisen noch zwei technische Lemmata, die uns in späteren Kapiteln nützen.

\begin{lemma}
    \label{lemma:outlier_sigma_zero}
    Bezeichnen wir für $n \in \N$ die erste $n$-te Einheitswurzel mit
    $\wn \defeq e^{\frac{2 \pi i}{n}}$, so gilt
    \[
        \abs{\sigma_r(\wn^1, \dots, \wn^{n-1})} = 1
    \]
    für alle $r = 0, \dots, n-1$.
\end{lemma}
\begin{proof}
    Nach Lemma \ref{lemma:elementary_symmetric_polynomials_const_multiplication}
    gilt
    \[
        \abs{\sigma_r(-\wn^1, \dots, -\wn^{n-1})}
        = \abs{(-1)^r \sigma_r(\wn^1, \dots, \wn^{n-1})}
        = \abs{\sigma_r(\wn^1, \dots, \wn^{n-1})}.
    \]
    Nach Definition der elementarsymmetrischen Polynome ist
    $\sigma_r({-\wn^1, \dots, -\wn^{n-1}})$
    der ${n\!-\!r\!-\!1}$-te Koeffizient des Polynoms vom
    Grad $n-1$, das eindeutig durch die $n-2$ Nullstellen
    $\wn^k$, $k=1, \dots, n-1$ bestimmt ist.
    Wir zeigen, dass dies das Polynom $p(z) = \sum_{r=0}^{n-1} z^r$ ist.

    \noindent Tatsächlich ist $\wn^k = e^{2 \pi i k / n}$ für $k = 1, \dots, n-1$
    eine Nullstelle von $p(z)$, denn wegen $\wn^k \neq 1$ folgt mit Hilfe der
    geometrischen Reihe
    \[
        \begin{split}
            p(\wn^k)
            &= \sum_{r=0}^{n-1} \left( \wn^k \right)^r
            = \frac{1 - \left( \wn^k \right)^n}{1 - \left( \wn^k \right)}
            = \frac{1 - \left( \wn^n \right)^k}{1 - \left( \wn^k \right)}
            = 0.
        \end{split}
    \]
    Damit ist gezeigt, dass für $r=0, \dots, n-1$
    \[
        \abs{\sigma_r(\wn^1, \dots, \wn^{n-1})}
        = \abs{\sigma_r(-\wn^1, \dots, -\wn^{n-1})}
        = 1
    \]
    gilt.
\end{proof}

\begin{corollary}
    \label{corollary:outlier_sigma_zero_row_sum}
    Es gilt
    \[
        \sum_{r=0}^{n-1} \abs{\sigma_r(\wn^1, \dots, \wn^{n-1})} = n.
    \]
\end{corollary}

\section[Abschätzung der Betragssumme der elementarsymmetrischen Polynome]{Abschätzung von \boldmath $\sum_{k=0}^{n} \abs{\sigma_{k}}$}
\begin{lemma}[\cite{gautschi4}]
    \label{lemma:elementary_symmetric_polynomials_inequality}
    Sei $x = (x_1, \dots, x_n) \in \Cn$ wie zuvor.
    Dann gilt:
    \begin{equation}
        \label{eq:elementary_symmetric_polynomials_inequality}
        \sum_{k=0}^{n} \abs{\sigma_{k}(x)} \leq \prod_{k=1}^{n} \left( 1 + \abs{x_k} \right)
    \end{equation}
    Gleichheit gilt, wenn $x_k = r_k \cdot e^{i\varphi}$ für ein
    festes $\varphi \in \R$ und beliebige $r_k \in \R_+$,
    d.h. wenn alle $x_k$ auf einer Halbgeraden durch den Nullpunkt
    in der komplexen Ebene liegen.
\end{lemma}

\begin{remark}
    Für die letzte Aussage des Lemmas, lässt sich sogar Äquivalenz zeigen, d.h.
    Gleichheit gilt \emph{genau dann}, wenn $x_k = r_k \cdot e^{i\varphi}$ für
    ein festes $\varphi \in \R$ und beliebige $r_k \in \R_+$.
    Da diese Aussage jedoch für den Rest der Arbeit nicht von Bedeutung ist,
    beweisen wir nur die schwächere Form des Lemmas.
\end{remark}

\begin{proof}[Beweis von Lemma \ref{lemma:elementary_symmetric_polynomials_inequality}]
    Zum Beweis, wenden wir vollständige Induktion über $n \in \N$ und
    Lemma \ref{lemma:recursion_elementary_symmetric_polynomials} an.\\[0.5em]
    \textbf{Induktionsanfang ($n=1$):}
    Es ist $ (z+x_0) = \sigma_0(x_0) \cdot z^1 + \sigma_1(x_0) \cdot z^0$,
    also $\sigma_0(x_0) = 1$ und $\sigma_1(x_0) = x_0$.
    Damit ergibt sich: $\sum_{k=0}^{1} \abs{\sigma_k(x_0)} = \abs{1} + \abs{x_0} = 1 + \abs{x_0}$.\\[0.5em]
%
    \textbf{Induktionvoraussetzung:}
    Sei die Behauptung für $n-1 \in \N$ erfüllt.\\[0.5em]
%
    \textbf{Induktionsschritt ($n-1 \rightarrow n$):}
    Unter Verwendung von $\sigma_{0}(x_1, \dots, x_n) = 1$
    und $\sigma_{n}(x_1, \dots, x_n) = x_1 \cdots x_n$, folgt:
    % TODO: Fix this.
    %\begin{equation*}
      \begin{align*}
        \prod_{k=1}^{n} \left( 1 + \abs{x_k} \right)
        &= ( 1 + \abs{x_n} ) \cdot \prod_{k=1}^{n-1} \left( 1 + \abs{x_k} \right)\\
        &\overset{IV}{\geq}  ( 1 + \abs{x_n} ) \cdot \sum_{k=0}^{n-1} \abs{\sigma_{k}(x_1, \dots, x_{n-1})}\\
        &= \sum_{k=0}^{n-1} \abs{\sigma_{k}(x_1, \dots, x_{n-1})}
        + \sum_{k=1}^{n} \abs{x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})}\\
        &\geq \sum_{k=1}^{n-1} \abs{\sigma_{k}(x_1, \dots, x_{n-1}) + x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})} + \abs{\sigma_{0}(x_1, \dots, x_{n-1})}\\
        &+ \abs{x_n \cdot \sigma_{n-1}(x_1, \dots, x_{n-1})} \numberthis \label{eq:elementary_polynomials_inequality_proof}\\
        &\overset{(\ref{eq:recursion_elementary_symmetric_polynomials})}{=}
        \sum_{k=1}^{n-1} \abs{\sigma_{k}(x_1, \dots, x_{n})} + \abs{\sigma_{0}(x_1, \dots, x_{n})} + \abs{\sigma_{n}(x_1, \dots, x_n)}\\
        &= \sum_{k=0}^{n} \abs{\sigma_{k}(x_1, \dots, x_{n-1})}.
      \end{align*}
    %\end{equation*}

    Um den Spezialfall $x_k = r_k e^{i \varphi}$ der Behauptung zu beweisen,
    führen wir die Induktion analog, mit der neuen Induktionsbehauptung, dass
    Gleichheit gelte.  Es bleibt nur zu zeigen, dass die Ungleichheit
    (\ref{eq:elementary_polynomials_inequality_proof}) zur Gleichheit wird, d.h. dass
    $\sigma_{k}(x_1, \dots, x_{n-1})$ und $x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})$
    linear abhängig sind und die Dreiecksungleichung als Gleichung
    \[
        \abs{\sigma_{k}(x_1, \dots, x_{n-1})} + \abs{x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})}
        = \abs{\sigma_{k}(x_1, \dots, x_{n-1}) + x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})}
    \]
    gilt.
    Dazu verwenden wir Gleichung
    (\ref{eq:explicit_elementary_symmetric_polynomials}) und stellen fest, dass
    \begin{equation*}
        \begin{split}
            \sigma_{k}(x_1, \dots, x_{n-1})
            &= \sum_{1 \leq j_1 < \dots < j_k \leq n-1} x_{j_1} \cdots x_{j_k}\\
            &= \sum_{1 \leq j_1 < \dots < j_k \leq n-1} r_{j_1} e^{i\varphi} \cdots r_{j_k} e^{i\varphi}\\
            &= e^{k\cdot i\varphi} \cdot \left( \sum_{1 \leq j_1 < \dots < j_k \leq n-1} r_{j_1} \cdots r_{j_k} \right)
        \end{split}
    \end{equation*}
    und
    \begin{equation*}
        \begin{split}
            x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})
            &= \sum_{1 \leq j_1 < \dots < j_{k-1} \leq n-1} x_n \cdot x_{j_1} \cdots x_{j_{k-1}}\\
            &= \sum_{1 \leq j_1 < \dots < j_{k-1} \leq n-1} r_n e^{i\varphi} \cdot r_{j_1} e^{i\varphi} \cdots r_{j_{k-1}} e^{i\varphi}\\
            &= e^{k\cdot i\varphi} \cdot \left( \sum_{1 \leq j_1 < \dots < j_{k-1} \leq n-1} r_n \cdot r_{j_1} \cdots r_{j_{k-1}} \right)
        \end{split}
    \end{equation*}
    in dieselbe Richtung $e^{k\cdot i\varphi}$ zeigen, also linear abhängig sind, wie behauptet.
\end{proof}
