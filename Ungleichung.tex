\chapter{Eine Ungleichung für die Kondition von Vandermonde Matrizen}
In diesem Abschnitt sei ein $n$-elementiger Vektor
$z = (z_0, \dots, z_{n-1}) \in \Cn$ gegeben.  Die zugehörige Vandermonde-Matrix
sei mit $V \defeq \Vand{z}$ bezeichnet.  Ziel ist es, eine Abschätzung für die
Norm der Inversen $\norm{V^{-1}}$ und damit für die Kondition $\cond{V}$ zu
finden.

Zunächst gilt es, die Inverse der Vandermonde-Matrix zu bestimmen. Anschließend
folgt ein Einschub über die \emph{elementarsymmetrischen Polynome}, mit deren
Hilfe sich $\Vand{z}^{-1}$ explizit darstellen lässt. Es folgt der eigentliche
Beweis der Ungleichung und einige Beispiele von Vandermonde-Matrizen mit
reellen Stützstellen und deren Kondition.

\section{Inversion der Vandermonde-Matrix}
\begin{mydef}[Lagrange-Polynome]
    Für $ z = (z_0, \dots, z_{n-1}) \in \Cn $ definiere die
    \emph{Lagrange-Polynome} als
    \[
        l_j(z) = \prod_{\substack{r=0\\ r \neq j}}^{n-1} \frac{z - z_r}{z_j - z_r}, \; j = 0, \dots, n-1.
    \]
\end{mydef}

\begin{remark}
    Es gilt $l_j \in \Pi_{n-1}$, wobei $\Pi_{n-1}$ den Polynom-Raum vom Grad $n-1$ bezeichne.
    Einfaches Nachrechnen liefert $l_j(z_r) = \delta_{jr}$,
    wobei $\delta_{jr}$ das Kronecker-Delta bezeichne.
\end{remark}

Wegen $l_j \in \Pi_{n-1}$, können die Lagrange-Polynome ausmultipliziert als
\begin{equation}
    \label{eq:lagrange}
    l_j(z) = \sum_{r = 0}^{n-1} u_{jr} z^{r}
\end{equation}
mit Koeffizienten $u_{jr} \in \C$ geschrieben werden.
Es zeigt sich nun
\begin{lemma}
    Sei $z = (z_0, \dots, z_{n-1}) \in \Cn$ und seien
    $l_j$, $j = 0, \dots, n-1$ die zugehörigen Lagrange-Polynome
    mit Koeffizienten $u_{jr}$ wie oben.
    Dann ist die Koeffizienten-Matrix $U = (u_{jr})_{j,r = 0}^{n-1}$ die
    Inverse der Vandermonde-Matrix $V = \Vand{z} = (z_{j}^{k})_{k,j = 0}^{n-1}$.
\end{lemma}

\begin{proof}
    Betrachte das Gleichungssystem $V \alpha = y$ mit $\alpha = (\alpha_0,
    \dots, \alpha_{n-1}) \in \Cn$ und $y = (y_0, \dots, y_{n-1}) \in \Cn$.
    Zum Beweis muss die Gleichung $U y = \alpha$ gezeigt werden.
    Es gilt für $j = 0, \dots, n-1$
    \[
        \begin{split}
            \sum_{k=0}^{n-1} u_{jk} y_k &= \sum_{k=0}^{n-1} u_{jk} \sum_{r=0}^{n-1} z_r^k \alpha_r = \sum_{r=0}^{n-1} \alpha_r \sum_{k=0}^{n-1} u_{jk} z_r^k\\
                                        &= \sum_{r=0}^{n-1} \alpha_r l_j(z_r) = \sum_{r=0}^{n-1} \alpha_r \delta_{jr} = \alpha_j,
        \end{split}
    \]
    wie behauptet.
\end{proof}

\section{Elementarsymmetrische Polynome}
\begin{mydef}[Elementarsymmetrische Polynome]
    Für $x = (x_1, \dots, x_{n}) \in \Cn$, definiere die
    \emph{k-ten elementarsymmetrischen Polynome
    $\sigma_{k}(x) = \sigma_{k}(x_1, \dots, x_{n})$, $k = 0, \dots, n$
    in den $n$ Variablen $x_j$, $j = 1, \dots, n$} durch
    \[
        \prod_{k=1}^{n} (z + x_k)
        = \sum_{k=0}^{n} \sigma_{k}(x_1, \dots, x_{n}) z^{n-k}.
    \]
\end{mydef}

\begin{remark}
    Ausmultiplizieren des Polynoms und Koeffizientenvergleich liefert:
    \begin{equation}
        \label{eq:explicit_elementary_symmetric_polynonimals}
        \sigma_{k}(x_1, \dots, x_{n})
        = \sum_{1 \leq j_1 < \dots < j_k \leq n} x_{j_1} \cdots x_{j_k}
        = \sum_{1 \leq j_1 < \dots < j_k \leq n} \left( \prod_{r=1}^k x_{j_r} \right).
    \end{equation}
\end{remark}

\begin{lemma}
    \label{lemma:recursion_elementary_symmetric_polynomials}
    Die elementarsymmetrischen Polynome erfüllen die Rekursionsformel:
    \begin{equation}
        \label{eq:recursion_elementary_symmetric_polynomials}
        \sigma_k (x_1, \dots, x_{n}) = \sigma_k (x_1, \dots, x_{n-1}) + x_{n} \sigma_{k-1} (x_1, \dots, x_{n-1}).
    \end{equation}
\end{lemma}

\begin{proof}
    Verwende Gleichung (\ref{eq:explicit_elementary_symmetric_polynonimals})
    und teile die rechte Seite in zwei Gruppen von Summanden, je nachdem, ob
    $x_n$ ein Faktor im Summand ist:
    \begin{equation*}
        \begin{split}
            \sigma_k (x_1, \dots, x_{n})
            &= \sum_{1 \leq j_1 < \dots < j_k \leq n} x_{j_1} \cdots x_{j_k}\\
            &= \sum_{1 \leq j_1 < \dots < j_k \leq n-1} x_{j_1} \cdots x_{j_k}
            + \sum_{1 \leq j_1 < \dots < j_{k-1} \leq n-1} x_{j_1} \cdots x_{j_k} \cdot x_{n}\\
            &= \sigma_k (x_1, \dots, x_{n-1}) + x_{n} \sigma_{k-1} (x_1, \dots, x_{n-1}).
        \end{split}
    \end{equation*}
\end{proof}

\begin{lemma}
    \label{lemma:elementary_symmetric_polynomials_inequality}
    Es gilt:
    \begin{equation}
        \label{eq:elementary_symmetric_polynomials_inequality}
        \sum_{k=0}^{n} \abs{\sigma_{k}} \leq \prod_{k=1}^{n} \left( 1 + \abs{x_k} \right)
    \end{equation}
    Gleicheit gilt genau dann, wenn $x_k = r_k \cdot e^{i\varphi}$ für ein festes $\varphi \in \R$ und beliebige $r_k \in \R_+$.
\end{lemma}

\begin{proof}
    Zum Beweis, wende vollständige Induktion über $n \in \N$ und
    Lemma (\ref{lemma:recursion_elementary_symmetric_polynomials}) an.\\[0.5em]
    \textbf{Induktionsanfang ($n=1$):}
    Es ist $ (z+x_0) = \sigma_0(x_0) \cdot z^1 + \sigma_1(x_0) \cdot z^0$,
    also $\sigma_0(x_0) = 1$ und $\sigma_1(x_0) = x_0$.
    Damit ergibt sich: $\sum_{k=0}^{1} \abs{\sigma_k(x_0)} = \abs{1} + \abs{x_0} = 1 + \abs{x_0}$.\\[0.5em]
%
    \textbf{Induktionvoraussetzung:}
    Sei die Behauptung für $n-1 \in \N$ erfüllt.\\[0.5em]
%
    \textbf{Induktionsschritt ($n-1 \rightarrow n$):}
    Unter Verwendung von $\sigma_{0}(x_1, \dots, x_n) = 1$
    und $\sigma_{n}(x_1, \dots, x_n) = x_1 \cdots x_n$, folgt:
    % TODO: Fix this.
    %\begin{equation*}
      \begin{align*}
        \prod_{k=1}^{n} \left( 1 + \abs{x_k} \right)
        &= ( 1 + \abs{x_n} ) \cdot \prod_{k=1}^{n-1} \left( 1 + \abs{x_k} \right)\\
        &\overset{IV}{\geq}  ( 1 + \abs{x_n} ) \cdot \sum_{k=0}^{n-1} \abs{\sigma_{k}(x_1, \dots, x_{n-1})}\\
        &= \sum_{k=0}^{n-1} \abs{\sigma_{k}(x_1, \dots, x_{n-1})}
        + \sum_{k=1}^{n} \abs{x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})}\\
        &\geq \sum_{k=1}^{n-1} \abs{\sigma_{k}(x_1, \dots, x_{n-1}) + x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})} + \abs{\sigma_{0}(x_1, \dots, x_{n-1})}\\
        &+ \abs{x_n \cdot \sigma_{n-1}(x_1, \dots, x_{n-1})} \numberthis \label{eq:elementary_polynomials_inequality_proof}\\
        &\overset{(\ref{eq:recursion_elementary_symmetric_polynomials})}{=}
        \sum_{k=1}^{n-1} \abs{\sigma_{k}(x_1, \dots, x_{n})} + \abs{\sigma_{0}(x_1, \dots, x_{n})} + \abs{\sigma_{n}(x_1, \dots, x_n)}\\
        &= \sum_{k=0}^{n} \abs{\sigma_{k}(x_1, \dots, x_{n-1})}.
      \end{align*}
    %\end{equation*}
    Um den Spezialfall $x_k = r_k e^{i \varphi}$ der Behauptung zu beweisen,
    führt man die Induktion analog, mit der neuen Induktionsbehauptung, dass
    Gleichheit gelte.  Es bleibt nur zu zeigen, dass die Ungleichheit
    (\ref{eq:elementary_polynomials_inequality_proof}) zur Gleichheit wird, d.h. dass
    $\sigma_{k}(x_1, \dots, x_{n-1})$ und $x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})$
    linear abhängig sind und die Dreiecksungleichung als Gleichung
    \[
        \abs{\sigma_{k}(x_1, \dots, x_{n-1})} + \abs{x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})}
        = \abs{\sigma_{k}(x_1, \dots, x_{n-1}) + x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})}
    \]
    gilt.
    Dazu verwende Gleichung
    (\ref{eq:explicit_elementary_symmetric_polynonimals}) und stelle fest, dass
    \begin{equation*}
        \begin{split}
            \sigma_{k}(x_1, \dots, x_{n-1})
            &= \sum_{1 \leq j_1 < \dots < j_k \leq n-1} x_{j_1} \cdots x_{j_k}\\
            &= \sum_{1 \leq j_1 < \dots < j_k \leq n-1} r_{j_1} e^{i\varphi} \cdots r_{j_k} e^{i\varphi}\\
            &= e^{k\cdot i\varphi} \cdot \left( \sum_{1 \leq j_1 < \dots < j_k \leq n-1} r_{j_1} \cdots r_{j_k} \right)
        \end{split}
    \end{equation*}
    und
    \begin{equation*}
        \begin{split}
            x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})
            &= \sum_{1 \leq j_1 < \dots < j_{k-1} \leq n-1} x_n \cdot x_{j_1} \cdots x_{j_{k-1}}\\
            &= \sum_{1 \leq j_1 < \dots < j_{k-1} \leq n-1} r_n e^{i\varphi} \cdot r_{j_1} e^{i\varphi} \cdots r_{j_{k-1}} e^{i\varphi}\\
            &= e^{k\cdot i\varphi} \cdot \left( \sum_{1 \leq j_1 < \dots < j_{k-1} \leq n-1} r_n \cdot r_{j_1} \cdots r_{j_{k-1}} \right)
        \end{split}
    \end{equation*}
    in dieselbe Richtung $e^{k\cdot i\varphi}$ zeigen, also linear abhängig sind, wie behauptet.
\end{proof}

\begin{notation}
    Als abkürzende Schreibweise setze im Folgenden für gegebene $x_1, \dots, x_{n} \in \C$
    \[
        \sigma_{k}^{j} \defeq \sigma_{k}(x_1, \dots, x_{j-1}, x_{j+1}, \dots, x_{n}).
    \]
    Damit ist $\sigma_{k}^{j}$ das k-te elementarsymmetrische Polynom in den
    $n-1$ Variablen $x_r$, $r \in \{ 1, \dots, n \} \setminus \{ j \}$.
\end{notation}

\begin{remark}
    %TODO: Remark vs. lemma? More verbose proof?
    Unter Verwendung von Gleichung (\ref{eq:lagrange}) und den
    elementarsymmetrischen Polynomen, kann eine explizite Darstellung der
    Inversen der Vandermonde-Matrix angegeben werden.
    Dazu schreibe für $j = 0, \dots, n-1$
    \begin{equation*}
        \sum_{r = 0}^{n-1} u_{jr} z^{r}
        = l_j(z)
        = \prod_{\substack{r=0\\ r \neq j}}^{n-1} \frac{z - z_r}{z_j - z_r}
        = \Pi_j \cdot \prod_{\substack{r=0\\ r \neq j}}^{n-1} \left( z - z_r \right)
    \end{equation*}
    mit
    \begin{equation}
        \label{def:pi_j}
        \Pi_j = \prod_{\substack{r=0\\ r \neq j}}^{n-1} \left( z_j - z_r \right)^{-1}.
    \end{equation}
    Koeffizientenvergleich liefert dann für $j, r = 0, \dots, n-1$
    \begin{equation}
        \label{eq:explicit_inverse_vandermonde}
        u_{jr} = (-1)^{r} \Pi_j \sigma_{r}^{j}.
    \end{equation}
\end{remark}

\section{Beweis der Ungleichung}

\begin{theorem}
  Seien $z_0, \dots, z_{n-1} \in \C$ paarweise verschieden.
  Es gilt:
  \begin{equation}
    \label{eq:inverse_vandermonde_inequality}
    \max_{j = 0, \dots, n-1} \left( \prod_{\substack{k = 0\\ k \neq j}}^{n-1} \frac{\max(1, \abs{z_k})}{\abs{z_j - z_k}} \right)
    < \norm{V^{-1}}_{\infty}
    \leq \max_{j = 0, \dots, n-1} \left( \prod_{\substack{k = 0\\ k \neq j}}^{n-1} \frac{1 + \abs{z_k}}{\abs{z_j - z_k}} \right).
  \end{equation}
  Gleichheit für die obere Grenze gilt genau dann, wenn $z_k = r_k \cdot e^{i \varphi}$
  für ein festes $\varphi \in \R$ und $r_k \in \R_{+}$, $k = 0, \dots, n-1$ gilt.
\end{theorem}

\begin{proof}
    Der Beweis orientiert sich an der Beweisskizze aus
    \cite[S. 196-197]{gautschi1}.
    Der Beweis der oberen Schranke von
    (\ref{eq:inverse_vandermonde_inequality}) erfolgt unter Verwendung der
    expliziten Darstellung von $V^{-1}$ aus
    (\ref{eq:explicit_inverse_vandermonde}) und der Ungleichung über
    elementarsymmetrische Polynome aus Lemma
    (\ref{lemma:elementary_symmetric_polynomials_inequality}).
    Es gilt:
    \begin{equation*}
        \begin{split}
            \norm{V^{-1}}_{\infty}
            &= \max_{k=0, \dots, n-1} \sum_{j=0}^{n-1} \abs{u_{kj}}\\
            &\overset{(\ref{eq:explicit_inverse_vandermonde})}{=}
              \max_{k=0, \dots, n-1} \sum_{j=0}^{n-1} \abs{(-1)^{r} \Pi_k \sigma_{j}^{k}}
            = \max_{k=0, \dots, n-1} \abs{\Pi_k} \sum_{j=0}^{n-1} \abs{\sigma_{j}^{k}}\\
            &\overset{(\ref{eq:elementary_symmetric_polynomials_inequality})}{\leq}
              \max_{k=0, \dots, n-1} \abs{\Pi_k} \prod_{\substack{r=0\\ r \neq k}}^{n-1} \left( 1 + \abs{z_r} \right)
            \overset{(\ref{def:pi_j})}{=}
              \max_{k=0, \dots, n-1} \prod_{\substack{r=0\\ r \neq k}}^{n-1} \frac{1 + \abs{z_r}}{\abs{z_k - z_r}}
        \end{split}
    \end{equation*}
\end{proof}

\section{Vandermonde Matrizen mit reellen Stützstellen}
