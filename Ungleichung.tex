\chapter{Eine Ungleichung für die Kondition von Vandermonde Matrizen}
In diesem Abschnitt sei ein $n$-elementiger Vektor $z = (z_0, \dots, z_{n-1})
\in \Cn$ gegeben.  Die zugehörige Vandermonde-Matrix sei mit $V \defeq
\Vand{z}$ bezeichnet.  Ziel ist es, eine Abschätzung der Zeilensummennorm der
inversen Vandermonde-Matrix $\norm{V^{-1}}_{\infty}$ und damit eine Abschätzung der
Kondition $\cond{V}$ zu finden.

Zunächst gilt es die Inverse der Vandermonde-Matrix zu bestimmen. Anschließend
betrachten wir die \emph{elementarsymmetrischen Polynome}, mit deren
Hilfe sich $\Vand{z}^{-1}$ explizit darstellen lässt. Es folgt der eigentliche
Beweis der Abschätzung und einige Beispiele von Vandermonde-Matrizen mit
reellen Stützstellen und deren Kondition.

\section{Inversion der Vandermonde-Matrix}
Es werden zunächst die \emph{Lagrange-Polynome} zu den Knoten
$z_0, \dots, z_{n-1}$ eingeführt, dessen Koeffizienten die Einträge der
Inversen der Vandermonde-Matrix sind.

\begin{mydef}[Lagrange-Polynome]
    Für $ z = (z_0, \dots, z_{n-1}) \in \Cn $ definiere die
    \emph{Lagrange-Polynome} als
    \[
        l_j(z) = \prod_{\substack{r=0\\ r \neq j}}^{n-1} \frac{z - z_r}{z_j - z_r}, \; j = 0, \dots, n-1.
    \]
\end{mydef}

\begin{remark}
    Es gilt $l_j \in \Pi_{n-1}$, wobei $\Pi_{n-1}$ den Raum der Polynome vom
    Grad $n-1$ bezeichne.  Einfaches Nachrechnen liefert
    $l_j(z_r) = \delta_{jr}$, wobei $\delta_{jr}$ das Kronecker-Delta
    sei.
\end{remark}

Wegen $l_j \in \Pi_{n-1}$, können die Lagrange-Polynome ausmultipliziert als
\begin{equation}
    \label{eq:lagrange}
    l_j(z) = \sum_{r = 0}^{n-1} u_{jr} z^{r}
\end{equation}
mit Koeffizienten $u_{jr} \in \C$ geschrieben werden.
Wir finden nun den folgenden Zusammenhang zwischen $V^{-1}$ und den
Koeffizienten der Lagrange-Polynome:

\begin{lemma}
    Sei $z = (z_0, \dots, z_{n-1}) \in \Cn$ und seien
    $l_j$, $j = 0, \dots, n-1$ die zugehörigen Lagrange-Polynome
    mit Koeffizienten $u_{jr}$ wie in (\ref{eq:lagrange}).
    Dann ist die Koeffizienten-Matrix $U = (u_{jr})_{j,r = 0}^{n-1}$ die
    Inverse der Vandermonde-Matrix $V = \Vand{z} = (z_{j}^{k})_{k,j = 0}^{n-1}$.
\end{lemma}

\begin{proof}
    Wir betrachten das Gleichungssystem $V \alpha = y$ mit
    $\alpha = (\alpha_0, \dots, \alpha_{n-1}) \in \Cn$
    und $y = (y_0, \dots, y_{n-1}) \in \Cn$.
    Zum Beweis muss die Gleichung $U y = \alpha$ für alle $\alpha,  y \in \Cn$
    gezeigt werden.
    Es gilt für $j = 0, \dots, n-1$
    \[
        \begin{split}
            \sum_{k=0}^{n-1} u_{jk} y_k &= \sum_{k=0}^{n-1} u_{jk} \sum_{r=0}^{n-1} z_r^k \alpha_r = \sum_{r=0}^{n-1} \alpha_r \sum_{k=0}^{n-1} u_{jk} z_r^k\\
                                        &= \sum_{r=0}^{n-1} \alpha_r l_j(z_r) = \sum_{r=0}^{n-1} \alpha_r \delta_{jr} = \alpha_j,
        \end{split}
    \]
    wie behauptet.
\end{proof}

\section{Elementarsymmetrische Polynome}
Um eine explizite Darstellung der Elemente von $V^{-1}$ zu finden, führen wir
die \emph{elementarsymmetrischen Polynome} ein.
Diese liefern ebenfalls die Grundlage der Abschätzung von
$\norm{V^{-1}}_\infty$ im nächsten Abschnitt.

\begin{mydef}[Elementarsymmetrische Polynome]
    Für $x = (x_1, \dots, x_{n}) \in \Cn$ definieren wir die
    \emph{k-ten elementarsymmetrischen Polynome
    $\sigma_{k}(x) = \sigma_{k}(x_1, \dots, x_{n})$, $k = 0, \dots, n$
    in den $n$ Variablen $x_j$, $j = 1, \dots, n$} durch
    \[
        \prod_{k=1}^{n} (z + x_k)
        = \sum_{k=0}^{n} \sigma_{k}(x_1, \dots, x_{n}) z^{n-k}.
    \]
\end{mydef}

\begin{remark}
    Ausmultiplizieren des Polynoms und Koeffizientenvergleich liefern:
    \begin{equation}
        \label{eq:explicit_elementary_symmetric_polynomials}
        \sigma_{k}(x_1, \dots, x_{n})
        = \sum_{1 \leq j_1 < \dots < j_k \leq n} x_{j_1} \cdots x_{j_k}
        = \sum_{1 \leq j_1 < \dots < j_k \leq n} \left( \prod_{r=1}^k x_{j_r} \right).
    \end{equation}
\end{remark}

\begin{lemma}
    \label{lemma:recursion_elementary_symmetric_polynomials}
    Die elementarsymmetrischen Polynome erfüllen die Rekursionsformel:
    \begin{equation}
        \label{eq:recursion_elementary_symmetric_polynomials}
        \sigma_k (x_1, \dots, x_{n}) = \sigma_k (x_1, \dots, x_{n-1}) + x_{n} \sigma_{k-1} (x_1, \dots, x_{n-1}).
    \end{equation}
\end{lemma}

\begin{proof}
    Wir verwenden Gleichung (\ref{eq:explicit_elementary_symmetric_polynomials})
    und teilen die rechte Seite in zwei Gruppen von Summanden, je nachdem, ob
    $x_n$ ein Faktor im Summand ist:
    \begin{equation*}
        \begin{split}
            \sigma_k (x_1, \dots, x_{n})
            &= \sum_{1 \leq j_1 < \dots < j_k \leq n} x_{j_1} \cdots x_{j_k}\\
            &= \sum_{1 \leq j_1 < \dots < j_k \leq n-1} x_{j_1} \cdots x_{j_k}
            + \sum_{1 \leq j_1 < \dots < j_{k-1} \leq n-1} x_{j_1} \cdots x_{j_k} \cdot x_{n}\\
            &= \sigma_k (x_1, \dots, x_{n-1}) + x_{n} \sigma_{k-1} (x_1, \dots, x_{n-1}).
        \end{split}
    \end{equation*}
\end{proof}

\begin{lemma}
    \label{lemma:elementary_symmetric_polynomials_inequality}
    Es gilt:
    \begin{equation}
        \label{eq:elementary_symmetric_polynomials_inequality}
        \sum_{k=0}^{n} \abs{\sigma_{k}} \leq \prod_{k=1}^{n} \left( 1 + \abs{x_k} \right)
    \end{equation}
    Gleichheit gilt, wenn $x_k = r_k \cdot e^{i\varphi}$ für ein
    festes $\varphi \in \R$ und beliebige $r_k \in \R_+$,
    d.h. wenn alle $x_k$ auf einer Halbgeraden durch den Nullpunkt
    in der komplexen Ebene liegen.
\end{lemma}

\begin{remark}
    Für die letzte Aussage des Lemmas, lässt sich sogar Äquivalenz zeigen, d.h.
    Gleichheit gilt \emph{genau dann}, wenn $x_k = r_k \cdot e^{i\varphi}$ für
    ein festes $\varphi \in \R$ und beliebige $r_k \in \R_+$.
    Da diese Aussage jedoch für den Rest der Arbeit nicht von Bedeutung ist,
    beweisen wir nur die schwächere Form des Lemmas.
\end{remark}

\begin{proof}[Beweis von Lemma \ref{lemma:elementary_symmetric_polynomials_inequality}]
    Zum Beweis, wenden wir vollständige Induktion über $n \in \N$ und
    Lemma \ref{lemma:recursion_elementary_symmetric_polynomials} an.\\[0.5em]
    \textbf{Induktionsanfang ($n=1$):}
    Es ist $ (z+x_0) = \sigma_0(x_0) \cdot z^1 + \sigma_1(x_0) \cdot z^0$,
    also $\sigma_0(x_0) = 1$ und $\sigma_1(x_0) = x_0$.
    Damit ergibt sich: $\sum_{k=0}^{1} \abs{\sigma_k(x_0)} = \abs{1} + \abs{x_0} = 1 + \abs{x_0}$.\\[0.5em]
%
    \textbf{Induktionvoraussetzung:}
    Sei die Behauptung für $n-1 \in \N$ erfüllt.\\[0.5em]
%
    \textbf{Induktionsschritt ($n-1 \rightarrow n$):}
    Unter Verwendung von $\sigma_{0}(x_1, \dots, x_n) = 1$
    und $\sigma_{n}(x_1, \dots, x_n) = x_1 \cdots x_n$, folgt:
    % TODO: Fix this.
    %\begin{equation*}
      \begin{align*}
        \prod_{k=1}^{n} \left( 1 + \abs{x_k} \right)
        &= ( 1 + \abs{x_n} ) \cdot \prod_{k=1}^{n-1} \left( 1 + \abs{x_k} \right)\\
        &\overset{IV}{\geq}  ( 1 + \abs{x_n} ) \cdot \sum_{k=0}^{n-1} \abs{\sigma_{k}(x_1, \dots, x_{n-1})}\\
        &= \sum_{k=0}^{n-1} \abs{\sigma_{k}(x_1, \dots, x_{n-1})}
        + \sum_{k=1}^{n} \abs{x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})}\\
        &\geq \sum_{k=1}^{n-1} \abs{\sigma_{k}(x_1, \dots, x_{n-1}) + x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})} + \abs{\sigma_{0}(x_1, \dots, x_{n-1})}\\
        &+ \abs{x_n \cdot \sigma_{n-1}(x_1, \dots, x_{n-1})} \numberthis \label{eq:elementary_polynomials_inequality_proof}\\
        &\overset{(\ref{eq:recursion_elementary_symmetric_polynomials})}{=}
        \sum_{k=1}^{n-1} \abs{\sigma_{k}(x_1, \dots, x_{n})} + \abs{\sigma_{0}(x_1, \dots, x_{n})} + \abs{\sigma_{n}(x_1, \dots, x_n)}\\
        &= \sum_{k=0}^{n} \abs{\sigma_{k}(x_1, \dots, x_{n-1})}.
      \end{align*}
    %\end{equation*}

    Um den Spezialfall $x_k = r_k e^{i \varphi}$ der Behauptung zu beweisen,
    führen wir die Induktion analog, mit der neuen Induktionsbehauptung, dass
    Gleichheit gelte.  Es bleibt nur zu zeigen, dass die Ungleichheit
    (\ref{eq:elementary_polynomials_inequality_proof}) zur Gleichheit wird, d.h. dass
    $\sigma_{k}(x_1, \dots, x_{n-1})$ und $x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})$
    linear abhängig sind und die Dreiecksungleichung als Gleichung
    \[
        \abs{\sigma_{k}(x_1, \dots, x_{n-1})} + \abs{x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})}
        = \abs{\sigma_{k}(x_1, \dots, x_{n-1}) + x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})}
    \]
    gilt.
    Dazu verwenden wir Gleichung
    (\ref{eq:explicit_elementary_symmetric_polynomials}) und stellen fest, dass
    \begin{equation*}
        \begin{split}
            \sigma_{k}(x_1, \dots, x_{n-1})
            &= \sum_{1 \leq j_1 < \dots < j_k \leq n-1} x_{j_1} \cdots x_{j_k}\\
            &= \sum_{1 \leq j_1 < \dots < j_k \leq n-1} r_{j_1} e^{i\varphi} \cdots r_{j_k} e^{i\varphi}\\
            &= e^{k\cdot i\varphi} \cdot \left( \sum_{1 \leq j_1 < \dots < j_k \leq n-1} r_{j_1} \cdots r_{j_k} \right)
        \end{split}
    \end{equation*}
    und
    \begin{equation*}
        \begin{split}
            x_n \cdot \sigma_{k-1}(x_1, \dots, x_{n-1})
            &= \sum_{1 \leq j_1 < \dots < j_{k-1} \leq n-1} x_n \cdot x_{j_1} \cdots x_{j_{k-1}}\\
            &= \sum_{1 \leq j_1 < \dots < j_{k-1} \leq n-1} r_n e^{i\varphi} \cdot r_{j_1} e^{i\varphi} \cdots r_{j_{k-1}} e^{i\varphi}\\
            &= e^{k\cdot i\varphi} \cdot \left( \sum_{1 \leq j_1 < \dots < j_{k-1} \leq n-1} r_n \cdot r_{j_1} \cdots r_{j_{k-1}} \right)
        \end{split}
    \end{equation*}
    in dieselbe Richtung $e^{k\cdot i\varphi}$ zeigen, also linear abhängig sind, wie behauptet.
\end{proof}

\begin{notation}
    Als abkürzende Schreibweise setzen wir im Folgenden für gegebene $x_1, \dots, x_{n} \in \C$
    \[
        \sigma_{k}^{j} \defeq \sigma_{k}(x_1, \dots, x_{j-1}, x_{j+1}, \dots, x_{n}).
    \]
    Damit ist $\sigma_{k}^{j}$ das k-te elementarsymmetrische Polynom in den
    $n-1$ Variablen $x_r$, $r \in \{ 1, \dots, n \} \setminus \{ j \}$.
\end{notation}

\begin{remark}
    %TODO: Remark vs. lemma? More verbose proof?
    Unter Verwendung von Gleichung (\ref{eq:lagrange}) und den
    elementarsymmetrischen Polynomen, kann eine explizite Darstellung der
    Inversen der Vandermonde-Matrix angegeben werden.
    Dazu schreiben wir für $j = 0, \dots, n-1$
    \begin{equation*}
        \sum_{r = 0}^{n-1} u_{jr} z^{r}
        = l_j(z)
        = \prod_{\substack{r=0\\ r \neq j}}^{n-1} \frac{z - z_r}{z_j - z_r}
        = \Pi_j \cdot \prod_{\substack{r=0\\ r \neq j}}^{n-1} \left( z - z_r \right)
    \end{equation*}
    mit
    \begin{equation}
        \label{def:pi_j}
        \Pi_j \defeq \prod_{\substack{r=0\\ r \neq j}}^{n-1} \left( z_j - z_r \right)^{-1}.
    \end{equation}
    Koeffizientenvergleich liefert dann für $j, r = 0, \dots, n-1$
    \begin{equation}
        \label{eq:explicit_inverse_vandermonde}
        u_{jr} = (-1)^{r} \Pi_j \sigma_{r}^{j}(z_0, \dots, z_{j-1}, z_{j+1}, \dots, z_{n-1}).
    \end{equation}
\end{remark}

% FIXME: Bad description.
\noindent Zum Abschluss dieses Abschnitts, beweisen wir noch ein Lemma, das später
nützlich sein wird.

\begin{lemma}
    Für $x = (x_1, \dots, x_n) \in \Cn$ und $\alpha \in \C$ gilt:
    \begin{equation}
        \label{eq:elementary_symmetric_polynomials_const_multiplication}
        \sigma_{k}(\alpha x)
        = \alpha^{k} \sigma_{k}(x)
    \end{equation}
\end{lemma}

\begin{proof}
    Unter Verwendung von Gleichung
    (\ref{eq:explicit_elementary_symmetric_polynomials})
    erhalten wir:
    \[
        \sigma_{k}(\alpha x)
        = \sum_{1 \leq j_1 < \dots < j_k \leq n} \alpha x_{j_1} \cdots \alpha x_{j_k}
        = \alpha^k \sum_{1 \leq j_1 < \dots < j_k \leq n} x_{j_1} \cdots x_{j_k}
        = \alpha^k \sigma_{k}(x).
    \]
\end{proof}

\begin{lemma}
    \label{lemma:inverse_vandermonde_const_multiplication}
    Sei $z = (z_0, \dots, z_{n-1}) \in \Cn$ und $\alpha \in \C$.
    Dann gilt:
    \begin{equation}
        \label{eq:inverse_vandermonde_const_multiplication}
        \Vand{\alpha z}^{-1}
        = \alpha^{-n+1} \cdot \Vand{z}^{-1} \cdot \diag{\alpha^0, \dots, \alpha^{n-1}}.
    \end{equation}
\end{lemma}

\begin{proof}
    Wir bezeichnen die Elemente von
    $V \defeq \Vand{z}$ mit $u_{jr} \in \C$, $j,r = 0, \dots, n-1$,
    und analog die Elemente von
    $\tilde{V} \defeq \Vand{\alpha z}$ mit
    $\tilde{u}_{jr} \in \C$, $j,r = 0, \dots, n-1$.
    Weiter sei
    \[
        \tilde{\Pi}_j
        = \prod_{\substack{r=0\\ r \neq j}}^{n-1} \left( \alpha z_j - \alpha z_r \right)^{-1}
        = \alpha^{-n+1} \prod_{\substack{r=0\\ r \neq j}}^{n-1} \left( z_j - z_r \right)^{-1}
        = \alpha^{-n+1} \Pi_j.
    \]

    \noindent Nach Gleichung (\ref{eq:explicit_inverse_vandermonde}) gilt:
    \begin{equation}
        \label{eq:inverse_vandermonde_const_multiplication}
        \begin{split}
            \tilde{u}_{jr}
            &\overset{(\ref{eq:explicit_inverse_vandermonde})}{=}
                (-1)^{r} \tilde{\Pi}_j \sigma_{r}^{j}(\alpha z_0, \dots, \alpha z_{j-1}, \alpha z_{j+1}, \dots, \alpha z_{n-1})\\
            &\overset{(\ref{eq:elementary_symmetric_polynomials_const_multiplication})}{=}
                \alpha^{r} (-1)^{r} \alpha^{-n+1} \Pi_j \sigma_{r}^{j}(z_0, \dots, z_{j-1}, z_{j+1}, \dots, z_{n-1})\\
            &= u_{jr} \alpha^{-n+r+1}.
        \end{split}
    \end{equation}

    \noindent Wie behauptet, können wir also
    $\tilde{V}^{-1} = \alpha^{-n+1} V^{-1} \cdot \diag{\alpha^{0}, \dots, \alpha^{n-1}}$ schreiben.

\end{proof}

\section{Abschätzung der Zeilensummennorm der inversen Vandermonde-Matrix}

Als Vorbereitung für den folgenden Satz wird noch ein weiteres Hilfslemma benötigt:

\begin{lemma}[\cite{gautschi2}]
    \label{lemma:polynom_coefficient_sum_inequality}
    Sei $p(z) = \sum_{j = 0}^{n} a_j z^j$ ein Polynom $n$-ten Grades mit
    $a_j \in \C$ für $j = 0, \dots, n$, $a_n \neq 0$ und Nullstellen
    $\zeta_r \in \C$, $r = 1, \dots, n$.
    Dann gilt
    \begin{equation}
        \label{eq:polynom_coefficient_sum_inequality}
        \sum_{j=0}^{n} \abs{a_j} \geq \abs{a_n} \prod_{r=1}^{n} \max \left(1, \abs{\zeta_r} \right).
    \end{equation}
    Gleichheit gilt genau dann, wenn $p(z) = a_n z^n$.
\end{lemma}

% TODO: Beautify this proof.
\begin{proof}
    Ohne Einschränkung können wir annehmen, dass die Nullstellen sortiert
    vorliegen, so dass
    \[
        \zeta_1 \leq \dots \leq \zeta_r \leq 1 < \zeta_{r+1} \leq \dots \leq \zeta_n
    \]
    gilt.
    Jensens Formel liefert dann
    \[
        \log \abs{a_0}
        = \log \abs{p(0)}
        = \sum_{k=1}^{r} \log \abs{\zeta_k} + \frac{1}{2\pi} \int_{0}^{2\pi} \log \abs{p\left( e^{i\varphi} \right)} d\varphi,
    \]
    oder äquivalent dazu
    \begin{equation}
        \label{eq:jensen_polynom}
        \abs{a_0} \prod_{k=1}^{r} \abs{\zeta_k}^{-1}
        = \exp\left( \frac{1}{2\pi} \int_{0}^{2\pi} \log \abs{p\left( e^{i\varphi} \right)} d\varphi \right).
    \end{equation}

    \noindent Mit Hilfe der Nullstellen $\zeta_j$ können wir $p$ als Produkt
    seiner Linearfaktoren darstellen
    \[
        p(z) = \sum_{j = 0}^{n} a_j z^j = a_n \prod_{k=1}^n (z-\zeta_k).
    \]

    \noindent Die Auswertung an $z=0$ liefert dann
    \[
        a_0 = p(0) = a_n (-1)^n \prod_{k=1}^n \zeta_k,
    \]
    so dass wir (\ref{eq:jensen_polynom}) vereinfacht darstellen können
    \[
        \abs{a_n} \prod_{k=r+1}^{n} \abs{\zeta_k}
        = \exp\left( \frac{1}{2\pi} \int_{0}^{2\pi} \log \abs{p\left( e^{i\varphi} \right)} d\varphi \right).
    \]

    \noindent Mit
    \begin{equation}
        \label{eq:jensen_integral_estimation}
        \exp\left( \frac{1}{2\pi} \int_{0}^{2\pi} \log \abs{p\left( e^{i\varphi} \right)} d\varphi \right)
        \leq \max_{0 \leq \varphi \leq 2\pi} \abs{p\left( e^{i\varphi} \right) }
        = \max_{0 \leq \varphi \leq 2\pi} \abs{\sum_{j=0}^{n} a_j e^{i\varphi}}
        \leq \sum_{j=0}^{n} \abs{a_j}
    \end{equation}
    folgt nun wie behauptet
    \[
        \sum_{j=0}^n \abs{a_j} \geq \abs{a_n} \prod_{k=1}^n \max(1, \zeta_k).
    \]

    \noindent Im Fall, dass $p(z) = a_n x^n$ gilt, ist
    $ \sum_{j=0}^{n} \abs{a_j}
      = \abs{a_n} \prod_{r=1}^n \max(1, \abs{\zeta_r})
      = \abs{a_n} $
    trivialerweise erfüllt.

    \noindent Sei umgekehrt  ein beliebiges Polynom
    $p(z) = \sum_{j=0}^n a_j x^j$, $a_n \neq 0$ gegeben, das
    (\ref{eq:polynom_coefficient_sum_inequality}) mit
    Gleichheit erfüllt.
    Dies impliziert wegen (\ref{eq:jensen_integral_estimation}) insbesondere
    \[
        \exp\left( \frac{1}{2\pi} \int_{0}^{2\pi} \log \abs{p\left( e^{i\varphi} \right)} d\varphi \right)
        = \max_{0 \leq \varphi \leq 2\pi} \abs{p\left( e^{i\varphi} \right) }
        \eqqcolon M,
    \]
    und damit, dass für $0 \leq \varphi \leq 2 \pi$ der Betrag des Polynoms
    $\abs{p\left( e^{i\varphi} \right)}$ konstant $M$ ist.
    Wir betrachten das trigonometrische Polynom
    \[
        \abs{p\left( e^{i\varphi} \right)}^2
        = \sum_{k,j=0}^n a_k \conj{a}_j e^{i(k-j)\varphi}
        = \sum_{l=-n}^n c_l e^{i l \varphi}
    \]
    mit Koeffizienten\footnote{Hier sei $a_k = 0$ für $k<0$ und $k > n$.}
    \[
        c_l = \sum_{-\infty}^\infty a_k \conj{a}_{k-l}, \; c_{-l} = \conj{c_l}.
    \]
    Wegen $\abs{p\left( e^{i\varphi} \right)}^2 \equiv M^2$, muss $c_j = 0$ für
    $j > 0$ und $c_0 = M^2$ gelten.
    Wir erhalten die Bedingungen
    \begin{equation*}
        \begin{split}
            0 &= c_n = a_n \conj{a}_0\\
            0 &= c_{n-1} = a_n \conj{a}_1 + a_{n-1} \conj{a}_0\\
              &\dots\\
            0 &= c_1 = a_n \conj{a}_{n-1} + \dots + a_{1} \conj{a}_0\\
            M^2 &= c_0 = a_n^2 + \dots + a_0^2.
        \end{split}
    \end{equation*}
    Wegen $a_n \neq 0$ liefert die erste Bedingung $a_0 = 0$.
    Einsetzen von $a_0 = 0$ in die zweite Bedingung liefert aus dem gleichen
    Grund $a_1 = 0$ und induktiv folgen $a_j = 0$ für $j < n$.
    Schließlich zeigt die letzte Gleichung $M^2 = a_n^2$, so dass $p$ wie
    behauptet ein Polynom von der Form $p(z) = a_n z^n$ ist.

\end{proof}

Nun können wir eine Abschätzung der Zeilensummennorm der inversen
Vandermonde-Matrix angegeben und beweisen:
\begin{theorem}
  \label{thm:inverse_vandermonde_inequality}
  Seien $z_0, \dots, z_{n-1} \in \C$ paarweise verschieden.
  Es gilt:
  \begin{equation}
    \label{eq:inverse_vandermonde_inequality}
    \max_{j = 0, \dots, n-1} \left( \prod_{\substack{k = 0\\ k \neq j}}^{n-1} \frac{\max(1, \abs{z_k})}{\abs{z_j - z_k}} \right)
    < \norm{V^{-1}}_{\infty}
    \leq \max_{j = 0, \dots, n-1} \left( \prod_{\substack{k = 0\\ k \neq j}}^{n-1} \frac{1 + \abs{z_k}}{\abs{z_j - z_k}} \right).
  \end{equation}
  Gleichheit für die obere Grenze gilt, wenn $z_k = r_k \cdot e^{i \varphi}$
  für ein festes $\varphi \in \R$ und $r_k \in \R_{+}$, $k = 0, \dots, n-1$ gilt.
\end{theorem}

\begin{proof}
    Der Beweis orientiert sich an der Beweisskizze aus
    \cite[S. 196-197]{gautschi1}.
    Die obere Schranke von (\ref{eq:inverse_vandermonde_inequality}) sieht man
    unter Verwendung der expliziten Darstellung von $V^{-1}$ aus
    (\ref{eq:explicit_inverse_vandermonde}) und der Ungleichung über
    elementarsymmetrische Polynome
    (\ref{eq:elementary_symmetric_polynomials_inequality}) aus Lemma
    \ref{lemma:elementary_symmetric_polynomials_inequality} ein.
    Es gilt:
    \begin{equation*}
        \begin{split}
            \norm{V^{-1}}_{\infty}
            &= \max_{j=0, \dots, n-1} \sum_{r=0}^{n-1} \abs{u_{jr}}\\
            &\overset{(\ref{eq:explicit_inverse_vandermonde})}{=}
              \max_{j=0, \dots, n-1} \sum_{r=0}^{n-1} \abs{(-1)^{r} \Pi_j \sigma_{r}^{j}}
            = \max_{j=0, \dots, n-1} \abs{\Pi_j} \sum_{r=0}^{n-1} \abs{\sigma_{r}^{j}}\\
            &\overset{(\ref{eq:elementary_symmetric_polynomials_inequality})}{\leq}
              \max_{j=0, \dots, n-1} \abs{\Pi_j} \prod_{\substack{k=0\\ k \neq j}}^{n-1} \left( 1 + \abs{z_k} \right)
            \overset{(\ref{def:pi_j})}{=}
              \max_{j=0, \dots, n-1} \prod_{\substack{k=0\\ k \neq j}}^{n-1} \frac{1 + \abs{z_k}}{\abs{z_j - z_k}}.
        \end{split}
    \end{equation*}

    \noindent Für die untere Schranke wähle nun ein festes
    $j \in \{0, \dots, n-1\}$ und betrachte das Polynom
    \[
        p(z) = \sum_{k=0}^{n-1} a_k z^k
        \defeq \Pi_j \cdot \prod_{\substack{k=0\\ k \neq j}}^{n-1} \left( z - z_k \right)
        \overset{(\ref{eq:explicit_inverse_vandermonde})}{=} \sum_{k=0}^{n-1} (-1)^{n-1-k} \Pi_j \sigma_{n-1-k}^j z^k,
    \]
    d.h. die Koeffizienten von $p$ ergeben sich für $k = 0, \dots, n-1$ als
    \[
        a_k = (-1)^{n-1-k} \cdot \Pi_j \cdot \sigma_{n-1-k}^j.
    \]
    Offensichtlich hat $p$ die $n-1$ Nullstellen
    $\zeta_r \defeq z_r$ für $r \in \{0,\dots,n-1\} \setminus \{j\}$.
    Wegen $z_k \neq z_r$ für $k \neq r$, hat $p$ Nullstellen ungleich $0$ und
    es kann nicht $p(z) \equiv a_{n-1} z^{n-1}$ gelten.
    Damit kann die strikte Ungleichung
    (\ref{eq:polynom_coefficient_sum_inequality}) aus Lemma
    \ref{lemma:polynom_coefficient_sum_inequality} angewendet werden und es
    folgt
    \[
        \begin{split}
            \sum_{r=0}^{n-1} \abs{u_{jr}}
            &\overset{(\ref{eq:explicit_inverse_vandermonde})}{=}
                \sum_{r=0}^{n-1} \abs{(-1)^r \Pi_j \sigma_r^j}\\
            &= \sum_{k=0}^{n-1} \abs{a_n}
            \overset{(\ref{eq:polynom_coefficient_sum_inequality})}{>}
                \abs{a_{n}} \prod_{\substack{r=0\\ r \neq j}}^{n-1} \max \left(1, \abs{\zeta_r} \right)\\
            &= \abs{\Pi_j} \underbrace{\abs{\sigma_0^j}}_{=1} \prod_{\substack{r=0\\ r \neq j}}^{n-1} \max \left(1, \abs{z_r} \right)
            = \prod_{\substack{r=0\\ r \neq j}}^{n-1} \frac{\max \left(1, \abs{z_r} \right)}{\abs{z_j - z_r}}.
        \end{split}
    \]

    \noindent Da diese Ungleichung für alle $j \in \{0, \dots, n-1\}$ erfüllt ist, folgt
    \[
        \max_{j = 0, \dots, n-1} \sum_{r=0}^{n-1} \abs{u_{jr}}
        > \max_{j = 0, \dots, n-1} \prod_{\substack{r=0\\ r \neq j}}^{n-1} \frac{\max \left(1, \abs{z_r} \right)}{\abs{z_j - z_r}},
    \]
    die Behauptung.
\end{proof}

\section{Vandermonde Matrizen mit reellen Stützstellen}
Als Anwendung von Satz \ref{thm:inverse_vandermonde_inequality} werden wir nun
einige ausgewählte Beispiele für Vandermonde-Matrizen mit reellen Stützstellen
betrachten und deren Kondition berechnen.
Die Beispiele wurden dabei aus \cite[S. 197-199]{gautschi1} entnommen.

Zunächst beweisen wir noch ein allgemeineres Lemma über die Zeilensummennorm
der Vandermonde-Matrix mit Stützstellen innerhalb des komplexen
Einheitskreises.
Betrachtet man nämlich nur Stützstellen $z_j \in \C$ mit $\abs{z_j} \leq 1$,
so gilt $\norm{\Vand{z}}_{\infty} = n$.
Somit muss in diesem Fall zur Bestimmung von $\cond{\Vand{z}}$ nur noch die
Norm der Inversen $\norm{\Vand{z}^{-1}}_{\infty}$ berechnet werden.
Insbesondere gilt dies für reelle Stützstellen
$x = (x_0, \dots, x_{n-1}) \in \Rn$ mit $\abs{x_j} \leq 1$.

\begin{lemma}
    Seien Stützstellen $z = (z_0, \dots, z_{n-1}) \in \Cn$
    mit $\abs{z_j} \leq 1$ für alle $j = 0, \dots, n-1$ gegeben.
    Dann gilt
    \[
        \norm{V(z)}_{\infty} = n.
    \]
\end{lemma}

\begin{proof}
    Wegen $\abs{z_j^k} \leq \abs{z_j^r}$ für alle $j = 0, \dots, n-1$ und $k > r$, folgt
    \[
        \norm{V(z)}_{\infty} = \max_{k=0,\dots,n-1} \left( \sum_{j=0}^{n-1} \abs{z_j^k} \right) = \sum_{j=0}^{n-1} \abs{z_j^0} = n.
    \]
\end{proof}

\subsection{Nicht-negative reelle Stützstellen}
Betrachte die Stützstellen $z_j = x_j \in \R_+$ für $j = 0, \dots, n-1$
und $x_k \neq x_j$ für $k \neq j$.
Diese erfüllen die Zusatzbedingung von Satz \ref{thm:inverse_vandermonde_inequality},
so dass bei der oberen Schranke von (\ref{eq:inverse_vandermonde_inequality})
Gleichheit gilt.
\begin{lemma}
    \label{lemma:nonnegative_real_nodes}
    Ist $V$ die zu den Stützstellen gehörige Vandermone-Matrix und definiert man
    \[
        p(z) \defeq \prod_{j=0}^{n-1} \left( z - x_j \right),
    \]
    so folgt
    \[
        \norm{V^{-1}}_{\infty} = \frac{ \abs{p(-1)}}{\min_{j=0, \dots, n-1} \{ (1+x_j) \abs{p^{\prime}(x_j)} \}}.
    \]
\end{lemma}

\begin{proof}
    Es gilt
    \[
        \abs{p(-1)} = \abs{\prod_{j=0}^{n-1} (-1 - x_j)} = \prod_{j=0}^{n-1} \left( 1 + x_j \right).
    \]
    Weiter ist nach der Produktregel
    \[
        p^{\prime}(z) = \sum_{k=0}^{n-1} \left( \prod_{\substack{j=0\\ j\neq k}}^{n-1} \left( z - x_j \right) \right),
    \]
    also
    \[
        \abs{p^{\prime}(x_k)} = \prod_{\substack{j=0\\ j\neq k}}^{n-1} \abs{ x_k - x_j }.
    \]
    Insgesamt folgt mit Satz \ref{thm:inverse_vandermonde_inequality}
    \[
        \begin{split}
            \frac{ \abs{p(-1)}}{\min_{j=0, \dots, n-1} \{ (1+x_j) \abs{p^{\prime}(x_j)} \}}
            = \frac{\prod_{k=0}^{n-1} \left( 1 + x_k \right)}{\min_{j=0, \dots, n-1} \{ (1+x_j) \prod_{\substack{k=0\\ k\neq j}}^{n-1} \abs{ x_j - x_k }\}}\\
            = \max_{j=0, \dots, n-1} \left( \prod_{\substack{k=0\\ k \neq j}}^{n-1} \frac{\left( 1 + x_k \right)}{\abs{ x_j - x_k }} \right)
            \overset{(\ref{eq:inverse_vandermonde_inequality})}{=} \norm{\Vand{x}^{-1}}_{\infty}.
        \end{split}
    \]
\end{proof}

\begin{example}[Harmonische Stützstellen]
    Seien die Stützstellen $x_k = \frac{1}{k}, \; k=1, \dots, n$ gegeben.
    Mit den Bezeichnungen wie in Lemma \ref{lemma:nonnegative_real_nodes} gilt dann
    \[
        \abs{p(-1)} = \prod_{k=1}^{n} \left( 1 + \frac{1}{k} \right) = n+1,
    \]
    wie eine einfache Induktion zeigt.
    Für $\delta_k \defeq (1 + x_k) \abs{ p^{\prime}(x_k) }$ gilt
    \[
        \begin{split}
            \delta_k &= \left( 1 + \frac{1}{k} \right) \prod_{\substack{r = 1\\ r\neq k}}^n \abs{ \frac{1}{k} - \frac{1}{r} }
                     = \left( \frac{k+1}{k} \right) \prod_{\substack{r = 1\\ r\neq k}}^n \abs{ \frac{r-k}{rk}}\\
                     &= \left( \frac{k+1}{k^n} \right) \frac{k}{n!} \prod_{\substack{r = 1\\ r\neq k}}^n \abs{ r-k }
                     = \left( \frac{k+1}{k^n} \right) \frac{k}{n!} (n-k)! \, (k-1)!\\
                     &= \left( \frac{(k+1)! \, (n-k)!}{k^n n!} \right).
        \end{split}
    \]
    Weiter folgt dann
    \[
        \min_{k=1, \dots, n} \delta_k \leq \delta_n = \frac{n+1}{n^n}
    \]
    und schließlich mit Lemma \ref{lemma:nonnegative_real_nodes}:
    \[
        \cond{V} = \norm{V}_\infty \cdot \norm{V^{-1}}_\infty \geq n \cdot (n+1) \frac{n^n}{n+1} = n^{n+1}.
    \]
\end{example}

\begin{example}[Äquidistante Stützstellen]
    Betrachte die Stützstellen $x_k = \frac{k-1}{n-1}, \; k=1, \dots, n$.
    Dann ist
    \[
        \begin{split}
            p(-1)
            &= \prod_{k=1}^{n} \left( 1 + \frac{k-1}{n-1} \right)
            = \prod_{k=1}^{n} \left( \frac{n + k - 2}{n-1} \right)\\
            &= \frac{1}{(n-1)^n} \frac{(2n-2)!\,}{(n-2)!\,}
            = \frac{(2n-2)!\,}{(n-1)^{n-1}(n-1)!\,}.
        \end{split}
    \]
    Mit $\delta_k \defeq (1 + x_k) \abs{ p^{\prime}(x_k) }$ gilt
    \[
        \begin{split}
            \delta_k
            &= \left( 1 + \frac{k-1}{n-1} \right) \cdot \prod_{\substack{j=1\\ j\neq k}}^{n} \abs{ \frac{k-j}{n-1} }
            = \frac{n+k-2}{(n-1)^{n}} \cdot \prod_{\substack{j=1\\ j\neq k}}^{n} \abs{ k-j }\\
            &= \frac{(n+k-2)(k-1)!\, (n-k)!\,}{(n-1)^{n}} .
        \end{split}
    \]
    ...\\ %TODO
    Insgesamt folgt
    \[
        \cond{\Vand{x}} \sim \frac{\sqrt{2}}{4\pi} \cdot 8^n \text{ für } n \rightarrow \infty.
    \]
\end{example}

\subsection{Symmetrische reelle Stützstellen}
In diesem Abschnitt seien nun reelle Stützstellen gegeben, die symmetrisch um
den Ursprung angeordnet sind.

\begin{lemma}[ohne Beweis]
    Sei $x = (x_1, \dots, x_n) \in \Rn$ mit $x_j + x_{n+1-j} = 0$ für $j = 1, \dots, n$.
    Dann gilt
    \[
        \norm{\Vand{x}^{-1}} = \frac{\abs{p(i)}}{\min_{x_k \geq 0} \left\{ \frac{1 + x_k^2}{1 + x_k} \abs{p^{\prime}(x_k)} \right\} }.
    \]
\end{lemma}
